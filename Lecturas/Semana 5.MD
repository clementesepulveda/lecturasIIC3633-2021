# Week 5: Combining Predictions for Accurate Recommender Systems

This 9-page paper might seem short, but it is still crammed with information and gives great insight into how a recommender system with 18 models is formed. It starts off with a quick and easy to follow introduction on why we would need recommender systems, and why we would want to ensemble them. After this, they explain some of the algorithms the team used for the Netflix Prize Contest, which were: KNN item-item, KNN user-user, SVD, AFM, SVD extended, RBM and GE. They give short but concise descriptions of each one, with information about their time complexity, their training time and also their memory management. After that, they talk about the blending. According to their research, combining different recommender models give a better result than if you had just made a single model. In this part, they explained Linear Regression, Binned Linear Regression, Neural Networks, Bagged Gradient Boosted Decision Trees, Kernel Ridge Regression Blending and K-Nearest Neighbors Blending. Just like in the last section, they explained how these algorithms worked and gave some extra information on how well they worked (according to theory and their big-O complexities). Subsequently, they showed how they worked in practice, compared to linear regression as a baseline estimate. Binned Linear Regression worked good as long as you didn't incorporate too many bins, meanwhile for neural networks it worked better if you added more and more neurons (which added a bit more time to train). Bagged gradient boosted decision trees worked good as well, but there were still parameters that depended on the data size. Kernel ridge regression blending was promising but couldn't handle too much information at once. KNN also had this promise, but it lacked on the 'promising' part, i.e., it was worse than the base error that they were looking for. Then begging with neural network, which also worked quiet well. 

What I liked most about this paper were the figures they showed. They showed information that showed how these models worked in practice in a way that was simple and understandable. For example, in figure 4 they showed a graph with the number of averaged KNN models vs RMSE on Ptest (with each line being trained on a different percentage of the data set). When we work with theory and exact numbers and percentages, we sometimes forget on how well things tend to be doing, but with this graph it was very intuitive to what was happening. Not only graphs, but also the tables were very easy to follow as well, like Table 1, which showed the RMSE, training time, prediction time and memory for different algorithms. It put into perspective how well some algorithms did compared to other ones and in what aspects. 

If you had had heard about combining predictions or not, this paper is great research on the implementation of this. It shows why combining predictors is superior to predictors on their own.
